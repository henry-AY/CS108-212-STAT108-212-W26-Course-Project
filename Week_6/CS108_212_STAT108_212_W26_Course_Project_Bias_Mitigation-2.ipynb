{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEovHtUoTG6K"
   },
   "source": [
    "# CS108/212 STAT108/212 W26 Course Project\n",
    "\n",
    "### Team Details\n",
    "\n",
    "- Teammate 1: Henry Yost\n",
    "- Teammate 2: Dmitry Sorokin\n",
    "- Teammate 3: Kyle Chahal\n",
    "- Teammate 4: Refugio Zepeda\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Milestone: Mitigating Bias\n",
    "For this project milestone, each teammate will implement bias mitigation strategies and assess pre and post bias mitigation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H61P2lQlNz1Q"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zcjl4O1GN24E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 5)) (1.6.1)\n",
      "Requirement already satisfied: ucimlrepo in /opt/anaconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (0.0.7)\n",
      "Collecting fairlearn (from -r ../requirements.txt (line 7))\n",
      "  Downloading fairlearn-0.13.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->-r ../requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->-r ../requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/anaconda3/lib/python3.13/site-packages (from ucimlrepo->-r ../requirements.txt (line 6)) (2025.4.26)\n",
      "Requirement already satisfied: narwhals>=1.14.0 in /opt/anaconda3/lib/python3.13/site-packages (from fairlearn->-r ../requirements.txt (line 7)) (1.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Downloading fairlearn-0.13.0-py3-none-any.whl (251 kB)\n",
      "Installing collected packages: fairlearn\n",
      "Successfully installed fairlearn-0.13.0\n"
     ]
    }
   ],
   "source": [
    "# [INSERT CODE HERE to install necessary packages]\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r ../requirements.txt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tXKU9aa5HPd"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CRQN7QJF5JUH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "## Add additional imports needed for your project here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V3zrVf11GUF"
   },
   "source": [
    "# Loading dataset\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Uimn7Sde1Jp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n",
      "No. of samples: 48842\n",
      "No. of features: 14\n"
     ]
    }
   ],
   "source": [
    "# Load your selected dataset\n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "\n",
    "# variable information \n",
    "print(adult.variables)\n",
    "\n",
    "# Making our data a pandas df\n",
    "adult_clean = pd.concat([X, y], axis=1)\n",
    "\n",
    "sensitive_feature_colname = ['age', 'sex', 'race', 'marital-status'] # sensitive feature name\n",
    "#age, sex, race, (marital status), \n",
    "\n",
    "# Make sensitive features-based group labels\n",
    "group_labels = adult_clean[sensitive_feature_colname]\n",
    "\n",
    "# Print some stats\n",
    "print(f\"No. of samples: {X.shape[0]}\")\n",
    "print(f\"No. of features: {X.shape[1]}\")\n",
    "#print(f\"Group Counts: {dict(collections.Counter(group_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq2dMuD87D0f"
   },
   "source": [
    "# Preparing dataset\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mq3wYB1H7CXM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples AFTER cleaning: 47876\n",
      "No. of features AFTER encoding: 12\n"
     ]
    }
   ],
   "source": [
    "# Some subset of following dataset preparation steps may be necessary depending on your dataset,\n",
    "# 1. Drop unnecessary features\n",
    "# 2. Handle missing data\n",
    "# 3. Encode categorical features\n",
    "# 4. Normalize numerical features\n",
    "# 5. Encode target (if your task is classification)\n",
    "\n",
    "\n",
    "\n",
    "#removing unwanted columns\n",
    "adult_clean = adult_clean.drop(columns = ['education', 'native-country'])\n",
    "#removing any empty values:\n",
    "adult_clean = adult_clean.dropna()\n",
    "#making income binary\n",
    "    #1 represents over 50 k\n",
    "mapping = {'>50K': 1, '<=50K': 0}\n",
    "adult_clean['income'] = adult_clean['income'].map(mapping)\n",
    "#updating sensitive labels\n",
    "group_labels = adult_clean[sensitive_feature_colname]\n",
    "#factorizing non-numeric data:\n",
    "\n",
    "adult_clean['workclass_num'], unique_labels = pd.factorize(adult_clean['workclass'])\n",
    "adult_clean['marital-status_num'], unique_labels = pd.factorize(adult_clean['marital-status'])\n",
    "adult_clean['occupation_num'], unique_labels = pd.factorize(adult_clean['occupation'])\n",
    "adult_clean['relationship_num'], unique_labels = pd.factorize(adult_clean['relationship'])\n",
    "adult_clean['race_num'], unique_labels = pd.factorize(adult_clean['race'])\n",
    "#For sex, male is 1, female is 0\n",
    "mapping = {'Male': 1, 'Female': 0}\n",
    "adult_clean['sex'] = adult_clean['sex'].map(mapping)\n",
    "\n",
    "\n",
    "X = adult_clean[['age', 'workclass_num', 'fnlwgt', 'education-num', 'marital-status_num', 'occupation_num', 'relationship_num', 'race_num', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "y = adult_clean[['income']]\n",
    "\n",
    "\n",
    "\n",
    "# Note: X and y have been modified before the following lines of code!\n",
    "print(f\"No. of samples AFTER cleaning: {X.shape[0]}\")\n",
    "assert X.shape[0] == y.shape[0] == group_labels.shape[0] ## Ensure that the target and group_labels have been updated if some samples were removed during cleaning.\n",
    "print(f\"No. of features AFTER encoding: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1xfwjUT3nJ0"
   },
   "source": [
    "# Getting training and testing sets\n",
    "\n",
    "Note: Train-test split is made **ONCE** to obtain the _training set_ and the _testing set_ and every teammate will use the training set to train their baseline model and test the trained model using the testing set. **NEVER** modify the testing set once it has been created.\n",
    "Therefore, the following code cell does not need to be edited.\n",
    "\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "udqlgotu5a5m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training samples: 38300\n",
      "No. of testing samples: 9576\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, \\\n",
    "  y_train, y_test, \\\n",
    "    group_labels_train, group_labels_test = train_test_split(X, y, group_labels,\n",
    "                                                             test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"No. of training samples: {X_train.shape[0]}\")\n",
    "print(f\"No. of testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Delete X, y and group_label variables to make sure they are not used later on.\n",
    "del X\n",
    "del y\n",
    "del group_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stfLke4NBA-B"
   },
   "source": [
    "# Setting up evaluation metrics\n",
    "Note: The same evaluation function will be used by all teammates.\n",
    "\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RxX61lMDA50u"
   },
   "outputs": [],
   "source": [
    "from fairlearn.metrics import equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# double importing just in case\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_test, y_pred, g_labels):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of your trained model on the testing set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : array-like\n",
    "    The true targets of the testing set.\n",
    "    y_pred : array-like\n",
    "    The predicted targets of the testing set.\n",
    "    g_labels : array-like\n",
    "    The group labels of the testing set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "    A dictionary containing the evaluation results.\n",
    "    \n",
    "    Example:\n",
    "    For classification task, the task-specific performance metrics like {'accuracy': <value>, 'f1_score': <value>, ...}\n",
    "    and fairness metrics like {'demographic_parity': <value>, 'equalized_odds': <value>, ...}.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # force them to be arrays just in case, so we don't have an error\n",
    "    y_test = np.asarray(y_test).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    g_labels = np.asarray(g_labels).ravel()\n",
    "    \n",
    "    # Note: These metrics will be calculated for - 1. the full testing set, 2. individual groups.\n",
    "    # Task-specific performance metrics\n",
    "\n",
    "    global_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Global Accuracy score of: {global_accuracy}\")\n",
    "    results[\"accuracy_overall\"] = global_accuracy\n",
    "\n",
    "    global_f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Global f1 score of: {global_f1}\")\n",
    "    results[\"f1_overall\"] = global_f1\n",
    "\n",
    "    results[\"accuracy_by_group\"] = {}\n",
    "    results[\"f1_by_group\"] = {}\n",
    "\n",
    "    for g in np.unique(g_labels):\n",
    "        mask = (g_labels == g)\n",
    "        y_test_g = y_test[mask]\n",
    "        y_pred_g = y_pred[mask]\n",
    "\n",
    "        results[\"accuracy_by_group\"][g] = accuracy_score(y_test_g, y_pred_g)\n",
    "        results[\"f1_by_group\"][g] = f1_score(y_test_g, y_pred_g, pos_label=1)\n",
    "    \n",
    "    # Fairness metric:\n",
    "    # The fairness metric we will be using is equalied odds, because: Equalized odds requires the TPR and FPR are equal accross all protected groups.\n",
    "\n",
    "    eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=g_labels, method='between_groups')\n",
    "    print(f\"Equalized Odds Difference: {eo_diff}\")\n",
    "    results['eo_diff'] = eo_diff\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My_giZbvpFEK"
   },
   "source": [
    "# Training baseline models (INDIVIDUAL CONTRIBUTION)\n",
    "_(minor modifications from previous milestone)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kK2P9uZnIKGf"
   },
   "outputs": [],
   "source": [
    "## A place to save all teammates's baseline results\n",
    "all_baseline_results = [] ## DO NOT EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "400FpWHZ_z0S"
   },
   "source": [
    "## Teammate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BjKUAk4I_4DQ"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     14\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m---> 15\u001b[0m     knn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     16\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     17\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/neighbors/_classification.py:239\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    220\u001b[0m )\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/neighbors/_base.py:478\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sklearn_tags__()\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[0;32m--> 478\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    480\u001b[0m             X,\n\u001b[1;32m    481\u001b[0m             y,\n\u001b[1;32m    482\u001b[0m             accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    483\u001b[0m             multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    484\u001b[0m             order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    485\u001b[0m             ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[1;32m    486\u001b[0m         )\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:1387\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m   1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1371\u001b[0m     X,\n\u001b[1;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1385\u001b[0m )\n\u001b[0;32m-> 1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:1397\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1397\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1398\u001b[0m         y,\n\u001b[1;32m   1399\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1400\u001b[0m         ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1401\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1402\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1403\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1404\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1405\u001b[0m     )\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1108\u001b[0m         array,\n\u001b[1;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "# Deciding to use a KNN model\n",
    "\n",
    "#normalizing training data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#finding optimal number of neighbors from 1 to 10, arbitrarily chosen, to limit processing time.\n",
    "optimal_n = 0\n",
    "optimal_n_acc = 0\n",
    "\n",
    "for i in range(1, 11):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "\n",
    "    if(0 == optimal_n):\n",
    "        optimal_n = i\n",
    "        optimal_n_acc = accuracy\n",
    "    elif(accuracy > optimal_n_acc):\n",
    "        optimal_n = i\n",
    "        optimal_n_acc = accuracy\n",
    "\n",
    "#Making the actual model:\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=optimal_n)\n",
    "KNN_model.fit(X_train, y_train)\n",
    "y_pred = KNN_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "# results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# # Save your results to all_baseline_results\n",
    "# results['teammate'] = 'Teammate 1'\n",
    "# results['experiment_type'] = 'baseline'\n",
    "# results['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "# results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "# all_baseline_results.append(results)\n",
    "\n",
    "# pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VYWPshD_zlw"
   },
   "source": [
    "## Teammate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgDw0Ta7_7FT"
   },
   "outputs": [],
   "source": [
    "# Select a model and train it on the training set\n",
    "# [INSERT YOUR CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Teammate 2'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bq--e_8_7o2"
   },
   "source": [
    "## Teammate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9DspEoL_-EV"
   },
   "outputs": [],
   "source": [
    "# Select a model and train it on the training set\n",
    "# [INSERT YOUR CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Teammate 3'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXm38f8ZABN3"
   },
   "source": [
    "## Teammate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-GVw7gOADZx"
   },
   "outputs": [],
   "source": [
    "# Select a model and train it on the training set\n",
    "# [INSERT YOUR CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Teammate 4'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Lq5X0Xb5eWA"
   },
   "source": [
    "# Mitigating Bias (INDIVIDUAL CONTRIBUTION)\n",
    "\n",
    "_(new in this milestone)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwfT5Uv9I0j6"
   },
   "outputs": [],
   "source": [
    "## A place to save all teammates' post-mitigation results\n",
    "all_mitigated_results = [] ## DO NOT EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmQrfFvn6E85"
   },
   "source": [
    "## Teammate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZhAOHFB5lzh"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 1'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "pprint(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X05x8kjr6KrH"
   },
   "source": [
    "### Teammate 1's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLB2ggUCBen_"
   },
   "source": [
    "## Teammate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtbctHpBBgna"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 2'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "print(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmwKk9f2BlaM"
   },
   "source": [
    "### Teammate 2's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUpxtxuUCF_I"
   },
   "source": [
    "## Teammate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KlNlq7lCInQ"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 3'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "print(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YN7oELoCKmF"
   },
   "source": [
    "### Teammate 3's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qNMlhiECMcD"
   },
   "source": [
    "## Teammate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-JCVhRdCPSw"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 4'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "print(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnJcnOevCSm-"
   },
   "source": [
    "### Teammate 4's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4LK5WRD7LvV"
   },
   "source": [
    "# Conclusions\n",
    "_(new in this milestone)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ZAgZJYf7F70N",
    "outputId": "42dcf95b-699c-4037-bb88-2c604648d111"
   },
   "outputs": [],
   "source": [
    "# Collect all the results in one table.\n",
    "overall_results = pd.concat([pd.DataFrame(all_baseline_results), pd.DataFrame(all_mitigated_results)])\n",
    "overall_results ## Note: The table displayed below in this starter notebook is for your reference, your team's table will be slightly different (e.g. different metrics, no.of sensitive attribute-based groups, actual values, etc.) upon successful completion of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds54O8vGGAEw"
   },
   "source": [
    "[Briefly describe overall findings and conclusions here. Which mitigation strategy resulted in most improvement? Which resulted in the least improvement? Visualize the results with some informative plots. (Hint: Use the `overall_results` table).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-8NLvIOsPL-"
   },
   "source": [
    "# References\n",
    "\n",
    "[List the references you used to complete this milestone here.]\n",
    "- Teammate 1:\n",
    "- Teammate 2:\n",
    "- Teammate 3:\n",
    "- Teammate 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV3LsIXqKAg1"
   },
   "source": [
    "# Disclosures\n",
    "\n",
    "[Disclose use of generative AI and similar tools here.]\n",
    "- Teammate 1:\n",
    "- Teammate 2:\n",
    "- Teammate 3:\n",
    "- Teammate 4:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7V3zrVf11GUF",
    "zq2dMuD87D0f",
    "h1xfwjUT3nJ0",
    "stfLke4NBA-B",
    "400FpWHZ_z0S",
    "_VYWPshD_zlw",
    "_Bq--e_8_7o2",
    "DXm38f8ZABN3",
    "ZLB2ggUCBen_",
    "ZUpxtxuUCF_I",
    "3qNMlhiECMcD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
