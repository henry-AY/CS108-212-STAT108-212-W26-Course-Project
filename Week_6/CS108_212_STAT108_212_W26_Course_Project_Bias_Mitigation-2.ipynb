{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEovHtUoTG6K"
   },
   "source": [
    "# CS108/212 STAT108/212 W26 Course Project\n",
    "\n",
    "### Team Details\n",
    "\n",
    "- Teammate 1: Henry Yost\n",
    "- Teammate 2: Dmitry Sorokin\n",
    "- Teammate 3: Kyle Chahal\n",
    "- Teammate 4: Refugio Zepeda\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Milestone: Mitigating Bias\n",
    "For this project milestone, each teammate will implement bias mitigation strategies and assess pre and post bias mitigation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H61P2lQlNz1Q"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zcjl4O1GN24E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r ../requirements.txt (line 1)) (3.10.8)\n",
      "Collecting seaborn (from -r ../requirements.txt (line 2))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r ../requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r ../requirements.txt (line 4)) (2.3.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r ../requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: ucimlrepo in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r ../requirements.txt (line 6)) (0.0.7)\n",
      "Collecting fairlearn (from -r ../requirements.txt (line 7))\n",
      "  Downloading fairlearn-0.13.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->-r ../requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->-r ../requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ucimlrepo->-r ../requirements.txt (line 6)) (2025.10.5)\n",
      "Collecting narwhals>=1.14.0 (from fairlearn->-r ../requirements.txt (line 7))\n",
      "  Downloading narwhals-2.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn->-r ../requirements.txt (line 5))\n",
      "  Downloading scipy-1.15.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\refug\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading fairlearn-0.13.0-py3-none-any.whl (251 kB)\n",
      "Downloading narwhals-2.17.0-py3-none-any.whl (444 kB)\n",
      "Downloading scipy-1.15.3-cp312-cp312-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.6/41.0 MB 13.7 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.2/41.0 MB 10.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.6/41.0 MB 10.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.9/41.0 MB 9.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 8.1/41.0 MB 9.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 8.4/41.0 MB 6.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 10.0/41.0 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 11.0/41.0 MB 6.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.3/41.0 MB 6.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.4/41.0 MB 6.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 17.3/41.0 MB 7.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.7/41.0 MB 8.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 25.2/41.0 MB 9.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 29.6/41.0 MB 9.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.6/41.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 37.0/41.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.1/41.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 10.8 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy, narwhals, seaborn, fairlearn\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.17.0\n",
      "    Uninstalling scipy-1.17.0:\n",
      "      Successfully uninstalled scipy-1.17.0\n",
      "Successfully installed fairlearn-0.13.0 narwhals-2.17.0 scipy-1.15.3 seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\refug\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# [INSERT CODE HERE to install necessary packages]\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tXKU9aa5HPd"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CRQN7QJF5JUH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from fairlearn.metrics import equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Add additional imports needed for your project here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V3zrVf11GUF"
   },
   "source": [
    "# Loading dataset\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Uimn7Sde1Jp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n",
      "No. of samples: 48842\n",
      "No. of features: 14\n"
     ]
    }
   ],
   "source": [
    "# Load your selected dataset\n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "\n",
    "# variable information \n",
    "print(adult.variables)\n",
    "\n",
    "# Making our data a pandas df\n",
    "adult_clean = pd.concat([X, y], axis=1)\n",
    "\n",
    "sensitive_feature_colname = ['age', 'sex', 'race', 'marital-status'] # sensitive feature name\n",
    "#age, sex, race, (marital status), \n",
    "\n",
    "# Make sensitive features-based group labels\n",
    "group_labels = adult_clean[sensitive_feature_colname]\n",
    "\n",
    "# Print some stats\n",
    "print(f\"No. of samples: {X.shape[0]}\")\n",
    "print(f\"No. of features: {X.shape[1]}\")\n",
    "#print(f\"Group Counts: {dict(collections.Counter(group_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq2dMuD87D0f"
   },
   "source": [
    "# Preparing dataset\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mq3wYB1H7CXM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples AFTER cleaning: 32561\n",
      "No. of features AFTER encoding: 12\n"
     ]
    }
   ],
   "source": [
    "# Some subset of following dataset preparation steps may be necessary depending on your dataset,\n",
    "# 1. Drop unnecessary features\n",
    "# 2. Handle missing data\n",
    "# 3. Encode categorical features\n",
    "# 4. Normalize numerical features\n",
    "# 5. Encode target (if your task is classification)\n",
    "\n",
    "\n",
    "\n",
    "#removing unwanted columns\n",
    "adult_clean = adult_clean.drop(columns = ['education', 'native-country'])\n",
    "#removing any empty values:\n",
    "adult_clean = adult_clean.dropna()\n",
    "#making income binary\n",
    "    #1 represents over 50 k\n",
    "mapping = {'>50K': 1, '<=50K': 0}\n",
    "adult_clean['income'] = adult_clean['income'].map(mapping)\n",
    "#updating sensitive labels\n",
    "group_labels = adult_clean[sensitive_feature_colname]\n",
    "#factorizing non-numeric data:\n",
    "\n",
    "adult_clean['workclass_num'], unique_labels = pd.factorize(adult_clean['workclass'])\n",
    "adult_clean['marital-status_num'], unique_labels = pd.factorize(adult_clean['marital-status'])\n",
    "adult_clean['occupation_num'], unique_labels = pd.factorize(adult_clean['occupation'])\n",
    "adult_clean['relationship_num'], unique_labels = pd.factorize(adult_clean['relationship'])\n",
    "adult_clean['race_num'], unique_labels = pd.factorize(adult_clean['race'])\n",
    "#For sex, male is 1, female is 0\n",
    "mapping = {'Male': 1, 'Female': 0}\n",
    "adult_clean['sex'] = adult_clean['sex'].map(mapping)\n",
    "\n",
    "adult_clean = adult_clean.dropna()\n",
    "\n",
    "X = adult_clean[['age', 'workclass_num', 'fnlwgt', 'education-num', 'marital-status_num', 'occupation_num', 'relationship_num', 'race_num', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "y = adult_clean[['income']]\n",
    "\n",
    "group_labels = adult_clean[sensitive_feature_colname]\n",
    "\n",
    "\n",
    "# Note: X and y have been modified before the following lines of code!\n",
    "print(f\"No. of samples AFTER cleaning: {X.shape[0]}\")\n",
    "assert X.shape[0] == y.shape[0] == group_labels.shape[0] ## Ensure that the target and group_labels have been updated if some samples were removed during cleaning.\n",
    "print(f\"No. of features AFTER encoding: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1xfwjUT3nJ0"
   },
   "source": [
    "# Getting training and testing sets\n",
    "\n",
    "Note: Train-test split is made **ONCE** to obtain the _training set_ and the _testing set_ and every teammate will use the training set to train their baseline model and test the trained model using the testing set. **NEVER** modify the testing set once it has been created.\n",
    "Therefore, the following code cell does not need to be edited.\n",
    "\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "udqlgotu5a5m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training samples: 26048\n",
      "No. of testing samples: 6513\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, \\\n",
    "  y_train, y_test, \\\n",
    "    group_labels_train, group_labels_test = train_test_split(X, y, group_labels,\n",
    "                                                             test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"No. of training samples: {X_train.shape[0]}\")\n",
    "print(f\"No. of testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Delete X, y and group_label variables to make sure they are not used later on.\n",
    "del X\n",
    "del y\n",
    "del group_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stfLke4NBA-B"
   },
   "source": [
    "# Setting up evaluation metrics\n",
    "Note: The same evaluation function will be used by all teammates.\n",
    "\n",
    "_(same as previous milestone, copy-paste)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RxX61lMDA50u"
   },
   "outputs": [],
   "source": [
    "# double importing just in case\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_test, y_pred, g_labels):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of your trained model on the testing set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : array-like\n",
    "    The true targets of the testing set.\n",
    "    y_pred : array-like\n",
    "    The predicted targets of the testing set.\n",
    "    g_labels : array-like\n",
    "    The group labels of the testing set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "    A dictionary containing the evaluation results.\n",
    "    \n",
    "    Example:\n",
    "    For classification task, the task-specific performance metrics like {'accuracy': <value>, 'f1_score': <value>, ...}\n",
    "    and fairness metrics like {'demographic_parity': <value>, 'equalized_odds': <value>, ...}.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # force them to be arrays just in case, so we don't have an error\n",
    "    y_test = np.asarray(y_test).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    g_labels = np.asarray(g_labels).ravel()\n",
    "    \n",
    "    # Note: These metrics will be calculated for - 1. the full testing set, 2. individual groups.\n",
    "    # Task-specific performance metrics\n",
    "\n",
    "    global_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Global Accuracy score of: {global_accuracy}\")\n",
    "    results[\"accuracy_overall\"] = global_accuracy\n",
    "\n",
    "    global_f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Global f1 score of: {global_f1}\")\n",
    "    results[\"f1_overall\"] = global_f1\n",
    "\n",
    "    results[\"accuracy_by_group\"] = {}\n",
    "    results[\"f1_by_group\"] = {}\n",
    "\n",
    "    for g in np.unique(g_labels):\n",
    "        mask = (g_labels == g)\n",
    "        y_test_g = y_test[mask]\n",
    "        y_pred_g = y_pred[mask]\n",
    "\n",
    "        results[\"accuracy_by_group\"][g] = accuracy_score(y_test_g, y_pred_g)\n",
    "        results[\"f1_by_group\"][g] = f1_score(y_test_g, y_pred_g, pos_label=1, zero_division=0)\n",
    "    \n",
    "    # Fairness metric:\n",
    "    # The fairness metric we will be using is equalied odds, because: Equalized odds requires the TPR and FPR are equal accross all protected groups.\n",
    "\n",
    "    eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=g_labels, method='between_groups')\n",
    "    print(f\"Equalized Odds Difference: {eo_diff}\")\n",
    "    results['eo_diff'] = eo_diff\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My_giZbvpFEK"
   },
   "source": [
    "# Training baseline models (INDIVIDUAL CONTRIBUTION)\n",
    "_(minor modifications from previous milestone)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kK2P9uZnIKGf"
   },
   "outputs": [],
   "source": [
    "## A place to save all teammates's baseline results\n",
    "all_baseline_results = [] ## DO NOT EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "400FpWHZ_z0S"
   },
   "source": [
    "## Teammate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BjKUAk4I_4DQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors:  1  Equalized Odds Diff:  0.7692307692307693\n",
      "Neighbors:  2  Equalized Odds Diff:  0.6153846153846154\n",
      "Neighbors:  3  Equalized Odds Diff:  0.8\n",
      "Neighbors:  4  Equalized Odds Diff:  0.6153846153846154\n",
      "Neighbors:  5  Equalized Odds Diff:  0.8\n",
      "Neighbors:  6  Equalized Odds Diff:  0.6923076923076923\n",
      "Neighbors:  7  Equalized Odds Diff:  0.7692307692307693\n",
      "Neighbors:  8  Equalized Odds Diff:  0.6923076923076923\n",
      "Neighbors:  9  Equalized Odds Diff:  0.8\n",
      "Neighbors:  10  Equalized Odds Diff:  0.7692307692307693\n",
      "Neighbors:  11  Equalized Odds Diff:  0.8461538461538461\n",
      "Neighbors:  12  Equalized Odds Diff:  0.7692307692307693\n",
      "Neighbors:  13  Equalized Odds Diff:  0.9230769230769231\n",
      "Neighbors:  14  Equalized Odds Diff:  0.9230769230769231\n",
      "Neighbors:  15  Equalized Odds Diff:  0.9230769230769231\n",
      "Neighbors:  16  Equalized Odds Diff:  0.7692307692307693\n",
      "Neighbors:  17  Equalized Odds Diff:  0.9230769230769231\n",
      "Neighbors:  18  Equalized Odds Diff:  0.7692307692307693\n",
      "Neighbors:  19  Equalized Odds Diff:  0.8461538461538461\n",
      "Neighbors:  20  Equalized Odds Diff:  0.7692307692307693\n",
      "Optimal number of neighbors:  2 . Optimal EO_diff:  0.6153846153846154\n",
      "Global Accuracy score of: 0.8225088284968525\n",
      "Global f1 score of: 0.5405405405405406\n",
      "Equalized Odds Difference: 0.6153846153846154\n",
      "{'accuracy_by_group': {'25_to_45_0_0': 0.8622448979591837,\n",
      "                       '25_to_45_0_1': 0.9818181818181818,\n",
      "                       '25_to_45_0_2': 0.8055555555555556,\n",
      "                       '25_to_45_0_3': 1.0,\n",
      "                       '25_to_45_0_4': 1.0,\n",
      "                       '25_to_45_1_0': 0.7528089887640449,\n",
      "                       '25_to_45_1_1': 0.845679012345679,\n",
      "                       '25_to_45_1_2': 0.7045454545454546,\n",
      "                       '25_to_45_1_3': 0.8260869565217391,\n",
      "                       '25_to_45_1_4': 0.8421052631578947,\n",
      "                       '46_to_65_0_0': 0.8519417475728155,\n",
      "                       '46_to_65_0_1': 0.9375,\n",
      "                       '46_to_65_0_2': 0.9,\n",
      "                       '46_to_65_0_3': 0.6666666666666666,\n",
      "                       '46_to_65_0_4': 1.0,\n",
      "                       '46_to_65_1_0': 0.6898395721925134,\n",
      "                       '46_to_65_1_1': 0.8076923076923077,\n",
      "                       '46_to_65_1_2': 0.7272727272727273,\n",
      "                       '46_to_65_1_3': 0.75,\n",
      "                       '46_to_65_1_4': 0.7777777777777778,\n",
      "                       'Over_65_0_0': 0.9552238805970149,\n",
      "                       'Over_65_0_1': 1.0,\n",
      "                       'Over_65_0_2': 1.0,\n",
      "                       'Over_65_0_4': 1.0,\n",
      "                       'Over_65_1_0': 0.8148148148148148,\n",
      "                       'Over_65_1_1': 0.9090909090909091,\n",
      "                       'Over_65_1_2': 0.75,\n",
      "                       'Over_65_1_3': 1.0,\n",
      "                       'Under_25_0_0': 0.9782608695652174,\n",
      "                       'Under_25_0_1': 1.0,\n",
      "                       'Under_25_0_2': 0.9166666666666666,\n",
      "                       'Under_25_0_3': 1.0,\n",
      "                       'Under_25_0_4': 1.0,\n",
      "                       'Under_25_1_0': 0.9802130898021308,\n",
      "                       'Under_25_1_1': 1.0,\n",
      "                       'Under_25_1_2': 0.9473684210526315,\n",
      "                       'Under_25_1_3': 1.0,\n",
      "                       'Under_25_1_4': 1.0},\n",
      " 'accuracy_overall': 0.8225088284968525,\n",
      " 'eo_diff': 0.6153846153846154,\n",
      " 'experiment_type': 'baseline',\n",
      " 'f1_by_group': {'25_to_45_0_0': 0.425531914893617,\n",
      "                 '25_to_45_0_1': 0.6666666666666666,\n",
      "                 '25_to_45_0_2': 0.2222222222222222,\n",
      "                 '25_to_45_0_3': 0.0,\n",
      "                 '25_to_45_0_4': 0.0,\n",
      "                 '25_to_45_1_0': 0.5310077519379846,\n",
      "                 '25_to_45_1_1': 0.5098039215686274,\n",
      "                 '25_to_45_1_2': 0.59375,\n",
      "                 '25_to_45_1_3': 0.5,\n",
      "                 '25_to_45_1_4': 0.0,\n",
      "                 '46_to_65_0_0': 0.41904761904761906,\n",
      "                 '46_to_65_0_1': 0.0,\n",
      "                 '46_to_65_0_2': 0.0,\n",
      "                 '46_to_65_0_3': 0.0,\n",
      "                 '46_to_65_0_4': 0.0,\n",
      "                 '46_to_65_1_0': 0.6063348416289592,\n",
      "                 '46_to_65_1_1': 0.5454545454545454,\n",
      "                 '46_to_65_1_2': 0.7272727272727273,\n",
      "                 '46_to_65_1_3': 0.0,\n",
      "                 '46_to_65_1_4': 0.5,\n",
      "                 'Over_65_0_0': 0.0,\n",
      "                 'Over_65_0_1': 0.0,\n",
      "                 'Over_65_0_2': 0.0,\n",
      "                 'Over_65_0_4': 0.0,\n",
      "                 'Over_65_1_0': 0.5454545454545454,\n",
      "                 'Over_65_1_1': 0.0,\n",
      "                 'Over_65_1_2': 0.0,\n",
      "                 'Over_65_1_3': 0.0,\n",
      "                 'Under_25_0_0': 0.16666666666666666,\n",
      "                 'Under_25_0_1': 0.0,\n",
      "                 'Under_25_0_2': 0.0,\n",
      "                 'Under_25_0_3': 0.0,\n",
      "                 'Under_25_0_4': 0.0,\n",
      "                 'Under_25_1_0': 0.38095238095238093,\n",
      "                 'Under_25_1_1': 0.0,\n",
      "                 'Under_25_1_2': 0.0,\n",
      "                 'Under_25_1_3': 0.0,\n",
      "                 'Under_25_1_4': 0.0},\n",
      " 'f1_overall': 0.5405405405405406,\n",
      " 'mitigation_strategy': 'NONE',\n",
      " 'predictor_model': 'KNN',\n",
      " 'teammate': 'Dmitry'}\n"
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "# Deciding to use a KNN model\n",
    "#\n",
    "#defining sensitive cols\n",
    "age_bins = [0, 25, 45, 65, 120]\n",
    "age_labels = ['Under_25', '25_to_45', '46_to_65', 'Over_65']\n",
    "\n",
    "# 2. Bin the ages from X_test ON THE FLY into a temporary variable\n",
    "binned_age_test = pd.cut(X_test['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# 3. Create your group_labels_test by combining the temporary binned ages \n",
    "# with the other numerical columns in X_test\n",
    "group_labels_test = binned_age_test.astype(str) + '_' + \\\n",
    "                    X_test['sex'].astype(str) + '_' + \\\n",
    "                    X_test['race_num'].astype(str)\n",
    "\n",
    "# sensitive_cols = ['age', 'sex', 'race_num', 'marital-status_num']\n",
    "\n",
    "# # Setting group_labels_test\n",
    "# group_labels_test = X_test[sensitive_cols].astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "#normalizing training data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#finding optimal number of neighbors from 1 to 10, arbitrarily chosen, to limit processing time.\n",
    "optimal_n = 0\n",
    "optimal_n_score = 0\n",
    "\n",
    "for i in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, np.asarray(y_train).ravel())\n",
    "    predictions = knn.predict(X_test)\n",
    "    model_score = eo_diff = equalized_odds_difference(y_test, predictions, sensitive_features=group_labels_test, method='between_groups')\n",
    "    #knn.score(X_test, y_test)\n",
    "\n",
    "    if(0 == optimal_n):\n",
    "        optimal_n = i\n",
    "        optimal_n_score = model_score\n",
    "    elif(model_score < optimal_n_score):\n",
    "        optimal_n = i\n",
    "        optimal_n_score = model_score\n",
    "\n",
    "    print('Neighbors: ', i, ' Equalized Odds Diff: ', model_score)\n",
    "\n",
    "#Making the actual model:\n",
    "KNN_model = KNeighborsClassifier(n_neighbors=optimal_n)\n",
    "KNN_model.fit(X_train, np.asarray(y_train).ravel())\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = KNN_model.predict(X_test)\n",
    "print(\"Optimal number of neighbors: \", optimal_n, \". Optimal EO_diff: \", optimal_n_score)\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Dmitry'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = 'KNN' #[INSERT MODEL NAME HERE]\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VYWPshD_zlw"
   },
   "source": [
    "## Teammate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dgDw0Ta7_7FT"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6513, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m y_pred = ... \u001b[38;5;66;03m# [INSERT CODE HERE]\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Evaluate testing set predictions using evaluate_model()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m results = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_labels_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Save your results to all_baseline_results\u001b[39;00m\n\u001b[32m     11\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mteammate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mTeammate 2\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(y_test, y_pred, g_labels)\u001b[39m\n\u001b[32m     31\u001b[39m g_labels = np.asarray(g_labels).ravel()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Note: These metrics will be calculated for - 1. the full testing set, 2. individual groups.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Task-specific performance metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m global_accuracy = \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGlobal Accuracy score of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33maccuracy_overall\u001b[39m\u001b[33m\"\u001b[39m] = global_accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:411\u001b[39m, in \u001b[36maccuracy_score\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[32m    410\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m y_type, y_true, y_pred, sample_weight = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type.startswith(\u001b[33m\"\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    416\u001b[39m     differing_labels = _count_nonzero(y_true - y_pred, xp=xp, device=device, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:108\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[33;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33;03msample_weight : array or None\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m type_true = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m type_pred = type_of_target(y_pred, input_name=\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:464\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    462\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    467\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [6513, 1]"
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "# [INSERT YOUR CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Teammate 2'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bq--e_8_7o2"
   },
   "source": [
    "## Teammate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9DspEoL_-EV"
   },
   "outputs": [],
   "source": [
    "# Select a model and train it on the training set\n",
    "# [INSERT YOUR CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Teammate 3'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXm38f8ZABN3"
   },
   "source": [
    "## Teammate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g-GVw7gOADZx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy score of: 0.8261937663135268\n",
      "Global f1 score of: 0.5574667709147771\n",
      "Equalized Odds Difference: 0.8333333333333334\n",
      "{'accuracy_by_group': {'25_to_45_0_0': 0.8469387755102041,\n",
      "                       '25_to_45_0_1': 0.9818181818181818,\n",
      "                       '25_to_45_0_2': 0.7777777777777778,\n",
      "                       '25_to_45_0_3': 1.0,\n",
      "                       '25_to_45_0_4': 1.0,\n",
      "                       '25_to_45_1_0': 0.7604698672114403,\n",
      "                       '25_to_45_1_1': 0.845679012345679,\n",
      "                       '25_to_45_1_2': 0.7159090909090909,\n",
      "                       '25_to_45_1_3': 0.9130434782608695,\n",
      "                       '25_to_45_1_4': 0.8947368421052632,\n",
      "                       '46_to_65_0_0': 0.8616504854368932,\n",
      "                       '46_to_65_0_1': 0.953125,\n",
      "                       '46_to_65_0_2': 0.9,\n",
      "                       '46_to_65_0_3': 0.8333333333333334,\n",
      "                       '46_to_65_0_4': 1.0,\n",
      "                       '46_to_65_1_0': 0.713903743315508,\n",
      "                       '46_to_65_1_1': 0.8333333333333334,\n",
      "                       '46_to_65_1_2': 0.7272727272727273,\n",
      "                       '46_to_65_1_3': 0.75,\n",
      "                       '46_to_65_1_4': 0.6666666666666666,\n",
      "                       'Over_65_0_0': 0.9253731343283582,\n",
      "                       'Over_65_0_1': 1.0,\n",
      "                       'Over_65_0_2': 1.0,\n",
      "                       'Over_65_0_4': 1.0,\n",
      "                       'Over_65_1_0': 0.725925925925926,\n",
      "                       'Over_65_1_1': 0.7272727272727273,\n",
      "                       'Over_65_1_2': 0.75,\n",
      "                       'Over_65_1_3': 1.0,\n",
      "                       'Under_25_0_0': 0.9804347826086957,\n",
      "                       'Under_25_0_1': 1.0,\n",
      "                       'Under_25_0_2': 0.9166666666666666,\n",
      "                       'Under_25_0_3': 1.0,\n",
      "                       'Under_25_0_4': 1.0,\n",
      "                       'Under_25_1_0': 0.9802130898021308,\n",
      "                       'Under_25_1_1': 0.9795918367346939,\n",
      "                       'Under_25_1_2': 0.9473684210526315,\n",
      "                       'Under_25_1_3': 1.0,\n",
      "                       'Under_25_1_4': 1.0},\n",
      " 'accuracy_overall': 0.8261937663135268,\n",
      " 'eo_diff': 0.8333333333333334,\n",
      " 'experiment_type': 'baseline',\n",
      " 'f1_by_group': {'25_to_45_0_0': 0.24050632911392406,\n",
      "                 '25_to_45_0_1': 0.5714285714285714,\n",
      "                 '25_to_45_0_2': 0.0,\n",
      "                 '25_to_45_0_3': 0.0,\n",
      "                 '25_to_45_0_4': 0.0,\n",
      "                 '25_to_45_1_0': 0.5295887662988967,\n",
      "                 '25_to_45_1_1': 0.4444444444444444,\n",
      "                 '25_to_45_1_2': 0.5901639344262295,\n",
      "                 '25_to_45_1_3': 0.8333333333333334,\n",
      "                 '25_to_45_1_4': 0.5,\n",
      "                 '46_to_65_0_0': 0.45714285714285713,\n",
      "                 '46_to_65_0_1': 0.4,\n",
      "                 '46_to_65_0_2': 0.0,\n",
      "                 '46_to_65_0_3': 0.6666666666666666,\n",
      "                 '46_to_65_0_4': 0.0,\n",
      "                 '46_to_65_1_0': 0.6595970307529162,\n",
      "                 '46_to_65_1_1': 0.6829268292682927,\n",
      "                 '46_to_65_1_2': 0.75,\n",
      "                 '46_to_65_1_3': 0.0,\n",
      "                 '46_to_65_1_4': 0.4,\n",
      "                 'Over_65_0_0': 0.2857142857142857,\n",
      "                 'Over_65_0_1': 0.0,\n",
      "                 'Over_65_0_2': 0.0,\n",
      "                 'Over_65_0_4': 0.0,\n",
      "                 'Over_65_1_0': 0.6105263157894737,\n",
      "                 'Over_65_1_1': 0.0,\n",
      "                 'Over_65_1_2': 0.0,\n",
      "                 'Over_65_1_3': 0.0,\n",
      "                 'Under_25_0_0': 0.0,\n",
      "                 'Under_25_0_1': 0.0,\n",
      "                 'Under_25_0_2': 0.0,\n",
      "                 'Under_25_0_3': 0.0,\n",
      "                 'Under_25_0_4': 0.0,\n",
      "                 'Under_25_1_0': 0.3157894736842105,\n",
      "                 'Under_25_1_1': 0.0,\n",
      "                 'Under_25_1_2': 0.0,\n",
      "                 'Under_25_1_3': 0.0,\n",
      "                 'Under_25_1_4': 0.0},\n",
      " 'f1_overall': 0.5574667709147771,\n",
      " 'mitigation_strategy': 'NONE',\n",
      " 'predictor_model': 'Logistic Regression',\n",
      " 'teammate': 'Refugio'}\n"
     ]
    }
   ],
   "source": [
    "# Select a model and train it on the training set\n",
    "# [INSERT YOUR CODE HERE]\n",
    "logreg = LogisticRegression(\n",
    "    C=1.0, \n",
    "    solver =\"lbfgs\",\n",
    "    max_iter= 1000 \n",
    ")\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results = evaluate_model(y_test, y_pred, group_labels_test)\n",
    "\n",
    "# Save your results to all_baseline_results\n",
    "results['teammate'] = 'Refugio'\n",
    "results['experiment_type'] = 'baseline'\n",
    "results['predictor_model'] = 'Logistic Regression'\n",
    "results['mitigation_strategy'] = 'NONE' ## DO NOT EDIT: This is pre-mitigation baseline\n",
    "all_baseline_results.append(results)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Lq5X0Xb5eWA"
   },
   "source": [
    "# Mitigating Bias (INDIVIDUAL CONTRIBUTION)\n",
    "\n",
    "_(new in this milestone)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwfT5Uv9I0j6"
   },
   "outputs": [],
   "source": [
    "## A place to save all teammates' post-mitigation results\n",
    "all_mitigated_results = [] ## DO NOT EDIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmQrfFvn6E85"
   },
   "source": [
    "## Teammate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZhAOHFB5lzh"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 1'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "pprint(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X05x8kjr6KrH"
   },
   "source": [
    "### Teammate 1's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLB2ggUCBen_"
   },
   "source": [
    "## Teammate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtbctHpBBgna"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 2'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "print(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmwKk9f2BlaM"
   },
   "source": [
    "### Teammate 2's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUpxtxuUCF_I"
   },
   "source": [
    "## Teammate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KlNlq7lCInQ"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 3'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "print(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YN7oELoCKmF"
   },
   "source": [
    "### Teammate 3's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qNMlhiECMcD"
   },
   "source": [
    "## Teammate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-JCVhRdCPSw"
   },
   "outputs": [],
   "source": [
    "# Implement your bias mitigation strategy\n",
    "## If you chose preprocessing, you will train a new version of your predictor model with new/modified inputs.\n",
    "## If you chose inprocessing, you will train a new version of your predictor with modified learning objective (loss function).\n",
    "## If you chose postprocessing, you will implement strategies to modify the predictions (y_pred) of the trained baseline predictor model from the previous milestone without training any new version of the predictor model.\n",
    "\n",
    "# [INSERT CODE HERE]\n",
    "\n",
    "# Make predictions on the testing set and store them in y_pred_mitigate\n",
    "y_pred_mitigated = ... # [INSERT CODE HERE]\n",
    "\n",
    "# Evaluate testing set predictions using evaluate_model()\n",
    "results_mitigated = evaluate_model(y_test, y_pred_mitigated, group_labels_test)\n",
    "\n",
    "# Save your results to all_mitigated_results\n",
    "results_mitigated['teammate'] = 'Teammate 4'\n",
    "results_mitigated['experiment_type'] = 'post-mitigation'\n",
    "results_mitigated['predictor_model'] = ... #[INSERT MODEL NAME HERE]\n",
    "results_mitigated['mitigation_strategy'] = ... #[INSERT STRATEGY TYPE HERE: 'preprocessing', 'inprocessing', 'postprocessing']\n",
    "all_mitigated_results.append(results_mitigated)\n",
    "\n",
    "print(results_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnJcnOevCSm-"
   },
   "source": [
    "### Teammate 4's Conclusions\n",
    "[Briefly describe findings and conclusions here. Compare post-mitigation results with baseline results for your model. What is the % improvement in performance post-mitigation?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4LK5WRD7LvV"
   },
   "source": [
    "# Conclusions\n",
    "_(new in this milestone)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ZAgZJYf7F70N",
    "outputId": "42dcf95b-699c-4037-bb88-2c604648d111"
   },
   "outputs": [],
   "source": [
    "# Collect all the results in one table.\n",
    "overall_results = pd.concat([pd.DataFrame(all_baseline_results), pd.DataFrame(all_mitigated_results)])\n",
    "overall_results ## Note: The table displayed below in this starter notebook is for your reference, your team's table will be slightly different (e.g. different metrics, no.of sensitive attribute-based groups, actual values, etc.) upon successful completion of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds54O8vGGAEw"
   },
   "source": [
    "[Briefly describe overall findings and conclusions here. Which mitigation strategy resulted in most improvement? Which resulted in the least improvement? Visualize the results with some informative plots. (Hint: Use the `overall_results` table).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-8NLvIOsPL-"
   },
   "source": [
    "# References\n",
    "\n",
    "[List the references you used to complete this milestone here.]\n",
    "- Teammate 1:\n",
    "- Teammate 2:\n",
    "- Teammate 3:\n",
    "- Teammate 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV3LsIXqKAg1"
   },
   "source": [
    "# Disclosures\n",
    "\n",
    "[Disclose use of generative AI and similar tools here.]\n",
    "- Teammate 1:\n",
    "- Teammate 2:\n",
    "- Teammate 3:\n",
    "- Teammate 4:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7V3zrVf11GUF",
    "zq2dMuD87D0f",
    "h1xfwjUT3nJ0",
    "stfLke4NBA-B",
    "400FpWHZ_z0S",
    "_VYWPshD_zlw",
    "_Bq--e_8_7o2",
    "DXm38f8ZABN3",
    "ZLB2ggUCBen_",
    "ZUpxtxuUCF_I",
    "3qNMlhiECMcD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
